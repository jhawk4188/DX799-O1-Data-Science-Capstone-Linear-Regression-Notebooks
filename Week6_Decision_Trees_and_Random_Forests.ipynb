{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b7dd55",
   "metadata": {},
   "source": [
    "# Week 6  Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c689368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths to uploaded datasets\n",
    "DATA_SMALL = \"/mnt/data/T_ONTIME_REPORTING.csv\"\n",
    "DATA_LARGE = \"/mnt/data/DelayData.csv\"\n",
    "\n",
    "# Utility: fast preview of a CSV\n",
    "def fast_preview(path, n=5):\n",
    "    print(f\"Previewing {path}\")\n",
    "    df = pd.read_csv(path, nrows=n)\n",
    "    display(df.head(n))\n",
    "    return df\n",
    "\n",
    "# Utility: chunked iterator for large CSV\n",
    "def chunk_reader(path, chunksize=100_000, usecols=None, dtype=None):\n",
    "    return pd.read_csv(path, chunksize=chunksize, usecols=usecols, dtype=dtype)\n",
    "\n",
    "# Utility: downsample large dataset for experiments\n",
    "def load_sample_from_large(n_rows=200_000, usecols=None):\n",
    "    # Stream chunks until we accumulate n_rows\n",
    "    rows = []\n",
    "    total = 0\n",
    "    for chunk in chunk_reader(DATA_LARGE, chunksize=100_000, usecols=usecols):\n",
    "        rows.append(chunk)\n",
    "        total += len(chunk)\n",
    "        if total >= n_rows:\n",
    "            break\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "    print(f\"Loaded sample of {len(df):,} rows from large file\")\n",
    "    return df\n",
    "\n",
    "# Quick sanity check previews\n",
    "_ = fast_preview(DATA_SMALL, n=5)\n",
    "_ = fast_preview(DATA_LARGE, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the same binary target setup\n",
    "usecols = [\n",
    "    \"arrdelay\",\"depdelay\",\"scheduledhour\",\"month\",\"dayofmonth\",\n",
    "    \"temperature\",\"windspeed\",\"raindummy\",\"snowdummy\",\n",
    "    \"marketshareorigin\",\"marketsharedest\",\"hhiorigin\",\"hhidest\"\n",
    "]\n",
    "df = load_sample_from_large(n_rows=250_000, usecols=usecols).dropna()\n",
    "df[\"target_delay15\"] = (df[\"arrdelay\"] > 15).astype(int)\n",
    "\n",
    "X = df.drop(columns=[\"arrdelay\",\"target_delay15\"]).values\n",
    "y = df[\"target_delay15\"].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Decision tree with simple tuning\n",
    "param_tree = {\"max_depth\":[5,10,15,20], \"min_samples_split\":[2,10,50]}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), param_tree, cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "tree_best = grid_tree.best_estimator_\n",
    "print(\"Tree best params:\", grid_tree.best_params_)\n",
    "\n",
    "# Random forest\n",
    "param_rf = {\"n_estimators\":[100,200], \"max_depth\":[10, None], \"min_samples_split\":[2,20]}\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), param_rf, cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "rf_best = grid_rf.best_estimator_\n",
    "print(\"RF best params:\", grid_rf.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "for model, name in [(tree_best,\"Decision Tree\"), (rf_best,\"Random Forest\")]:\n",
    "    proba = model.predict_proba(X_test)[:,1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(classification_report(y_test, pred))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, proba))\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "importances = rf_best.feature_importances_\n",
    "feat_names = df.drop(columns=[\"arrdelay\",\"target_delay15\"]).columns\n",
    "fi = pd.Series(importances, index=feat_names).sort_values(ascending=False).head(15)\n",
    "fi.to_frame(\"importance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe89c24",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Record top features and how they align with domain expectations. Discuss overfitting controls such as max depth, min samples split, and out of bag if used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
